[["distributions-of-quadratic-forms.html", "Chapter 4 Distributions of Quadratic Forms 4.1 QUADRATIC FORMS OF NORMAL RANDOM VECTORS 4.2 INDEPENDENCE 4.3 THE \\(\\boldsymbol{t}\\) AND \\(\\boldsymbol{F}\\) DISTRIBUTIONS 4.4 BHAT’S LEMMA 4.5 EXERCISES", " Chapter 4 Distributions of Quadratic Forms The distribution of the quadratic form \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}\\) is now derived when \\(\\mathbf{Y} \\sim \\mathbf{N}_{n}\\left(\\mathbf{0}, \\mathbf{I}_{n}\\right) .\\) Later, the distribution of \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y}\\) is developed when \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}(\\mathbf{0}, \\mathbf{\\Sigma})\\) for any positive definite \\(n \\times n\\) matrix \\(\\boldsymbol{\\Sigma}\\). 4.1 QUADRATIC FORMS OF NORMAL RANDOM VECTORS A chi-square random variable with \\(n\\) degrees of freedom and the noncentrality parameter \\(\\lambda\\) will be designated by \\(\\chi_{n}^{2}(\\lambda) .\\) Therefore, a central chi-square random variable with \\(n\\) degrees of freedom is denoted by \\(\\chi_{n}^{2}(\\lambda=0)\\) or \\(\\chi_{n}^{2}(0)\\). Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}\\left(\\mathbf{0}, \\mathbf{I}_{n}\\right)\\) then \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y} \\sim\\) \\(\\chi_{p}^{2}(\\lambda=0)\\) if and only if \\(\\mathbf{A}\\) is an \\(n \\times n\\) idempotent matrix of rank \\(p .\\) Proof. Proof. First assume \\(\\mathbf{A}\\) is an \\(n \\times n\\) idempotent matrix of rank \\(p\\). By Theorem 1.1.10, \\(\\mathbf{A}=\\mathbf{P P}^{\\prime}\\) where \\(\\mathbf{P}\\) is an \\(n \\times p\\) matrix of eigenvectors with \\(\\mathbf{P}^{\\prime} \\mathbf{P}=\\mathbf{I}_{p} .\\) Let the \\(p \\times 1\\) random vector \\(\\mathbf{X}=\\mathbf{P}^{\\prime} \\mathbf{Y}\\). By Theorem \\(2.1 .2\\) with \\(p \\times n\\) matrix \\(\\mathbf{B}=\\mathbf{P}^{\\prime}\\) and \\(p \\times 1\\) vector \\(\\mathbf{b}=\\mathbf{0}_{p \\times 1}, \\mathbf{X}=\\left(X_{1}, \\ldots, X_{p}\\right)^{\\prime} \\sim \\mathrm{N}_{p}\\left(\\mathbf{0}, \\mathbf{I}_{p}\\right) .\\) Therefore, by Example 1.3.2, Y’AY \\(=\\mathbf{Y}^{\\prime} \\mathbf{P} \\mathbf{P}^{\\prime} \\mathbf{Y}=\\mathbf{X}^{\\prime} \\mathbf{X}=\\sum_{i=1}^{p} X_{i}^{2} \\sim \\chi_{p}^{2}(\\lambda=0) .\\) Next assume that \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y} \\sim \\chi_{p}^{2}(\\lambda=0)\\). Therefore, the moment generating function of \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y}\\) is \\((1-2 t)^{-p / 2}\\). But the moment generating function of \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y}\\) is also defined as \\[\\begin{aligned} m_{\\mathbf{Y}^{\\prime} \\mathbf{A Y}}(t) &amp;=\\mathrm{E}\\left[e^{t \\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}}\\right] \\\\ &amp;=\\int \\cdots \\int(2 \\pi)^{-n / 2} e^{\\left[t \\mathbf{y}^{\\prime} \\mathbf{A} \\mathbf{y}-\\mathbf{y}^{\\prime} \\mathbf{y} / 2\\right]} d y_{1} \\ldots d y_{n} \\\\ &amp;=\\int \\cdots \\int(2 \\pi)^{-n / 2} e^{\\left[\\mathbf{y}^{\\prime}\\left(\\mathbf{I}_{n}-2 t \\mathbf{A}\\right) \\mathbf{y} / 2\\right]} d y_{1} \\ldots d y_{n} \\\\ &amp;=\\left|\\mathbf{I}_{n}-2 t \\mathbf{A}\\right|^{-1 / 2} \\end{aligned}\\] The final equality holds since the last integral equation is the integral of a multivariate normal distribution (without the Jacobian \\(\\left|\\mathbf{I}_{n}-2 t \\mathbf{A}\\right|^{1 / 2}\\) ) with mean vector \\(\\mathbf{0}_{n \\times 1}\\) and covariance matrix \\(\\left(\\mathbf{I}_{n}-2 t \\mathbf{A}\\right)^{-1}\\). The two forms of the moment generating function must be equal for all \\(t\\) in some neighborhood of zero. Therefore, \\[(1-2 t)^{-p / 2}=\\left|\\mathbf{I}_{n}-2 t \\mathbf{A}\\right|^{-1 / 2}\\] or \\[(1-2 t)^{p}=\\left|\\mathbf{I}_{n}-2 t \\mathbf{A}\\right|\\] Let \\(\\mathbf{Q}\\) be the \\(n \\times n\\) matrix of eigenvectors and \\(\\mathbf{D}\\) be the \\(n \\times n\\) diagonal matrix of eigenvalues of \\(\\mathbf{A}\\) where the eigenvalues are given by \\(\\lambda_{1}, \\ldots, \\lambda_{n} .\\) By Theorem \\(1.1 .3, \\mathbf{Q}^{\\prime} \\mathbf{A} \\mathbf{Q}=\\mathbf{D}, \\mathbf{Q}^{\\prime} \\mathbf{Q}=\\mathbf{I}_{n}\\), and \\[\\begin{aligned} \\left|\\mathbf{I}_{n}-2 t \\mathbf{A}\\right| &amp;=\\left(\\left|\\mathbf{Q}^{\\prime} \\mathbf{Q}\\right|\\right)\\left(\\left|\\mathbf{I}_{n}-2 t \\mathbf{A}\\right|\\right) \\\\ &amp;=\\left|\\mathbf{Q}^{\\prime}\\left(\\mathbf{I}_{n}-2 t \\mathbf{A}\\right) \\mathbf{Q}\\right| \\\\ &amp;=\\left|\\mathbf{I}_{n}-2 t \\mathbf{Q}^{\\prime} \\mathbf{A} \\mathbf{Q}\\right| \\\\ &amp;\\left.=\\mid \\mathbf{I}_{n}-2 t \\mathbf{D}\\right] \\\\ &amp;=\\prod_{i=1}^{n}\\left(1-2 t \\lambda_{i}\\right) \\end{aligned}\\] Therefore, \\((1-2 t)^{p}=\\prod_{i=1}^{n}\\left(1-2 t \\lambda_{i}\\right) .\\) The left side of the equation is a polynomial in \\(2 t\\) with highest power \\(p\\). The right side of the equation therefore must have highest power \\(p\\) in \\(2 t\\) also, implying that \\((n-p)\\) of the \\(\\lambda_{i}\\) ’s are zero. Thus, the equation becomes \\((1-2 t)^{p}=\\prod_{i=1}^{p}\\left(1-2 t \\lambda_{i}\\right)\\). Taking logarithms of each side and equating coefficients produces \\(1-2 t=1-2 t \\lambda_{i}\\) for \\(i=1, \\ldots, p\\). The solution to these equations is \\(\\lambda_{1}=\\cdots=\\lambda_{p}=1\\). Therefore, by Theorem 1.1.8, \\(\\mathbf{A}\\) is an idempotent matrix. ◻ Thus far we have concentrated on central chi-square random variables (i.e., \\(\\lambda=0\\) ). However, in general, if the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}\\left(\\mu, \\mathbf{I}_{n}\\right)\\) then \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y} \\sim \\chi_{p}^{2}(\\lambda)\\) where \\(\\mu\\) is any \\(n \\times 1\\) mean vector and the noncentrality parameter is given by \\(\\lambda = \\boldsymbol{\\mu}^{\\prime} \\mathbf{A} \\boldsymbol{\\mu} / 2\\). The next theorem considers the distribution of \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}\\) when \\(\\mathbf{Y} \\sim \\mathbf{N}_{n}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) and \\(\\boldsymbol{\\Sigma}\\) is a positive definite matrix of rank \\(n\\). Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}(\\mu, \\Sigma)\\) where \\(\\boldsymbol{\\Sigma}\\) is an \\(n \\times n\\) positive definite matrix of rank \\(n .\\) Then \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y} \\sim \\chi_{p}^{2}\\left(\\lambda=\\mu^{\\prime} \\mathbf{A} \\boldsymbol{\\mu} / 2\\right)\\) if and only if any of the following conditions are satisfied: (1) A\\(\\mathbf{\\Sigma}\\) (or \\(\\mathbf{\\Sigma A}\\) ) is an idempotent matrix of rank p or (2) \\(\\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A}=\\mathbf{A}\\) and \\(\\mathbf{A}\\) has rank \\(p\\). Proof. Proof. \\(\\quad\\) Let \\(\\mathbf{Z}=\\mathbf{T}^{-1}(\\mathbf{Y}-\\boldsymbol{\\mu})\\) where \\(\\boldsymbol{\\Sigma}=\\mathbf{T T}^{\\prime} .\\) By Theorem \\(2.1 .2\\) with \\(n \\times n\\) matrix \\(\\mathbf{B}=\\mathbf{T}^{-1}\\) and \\(n \\times 1\\) vector \\(\\mathbf{b}=-\\mathbf{T}^{-1} \\boldsymbol{\\mu}, \\mathbf{Z} \\sim \\mathbf{N}_{n}\\left(\\mathbf{0}, \\mathbf{I}_{n}\\right) .\\) Furthermore, \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y}=(\\mathbf{T Z}+\\mu)^{\\prime} \\mathbf{A}(\\mathbf{T Z}+\\mu)=\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)^{\\prime} \\mathbf{T}^{\\prime} \\mathbf{A T}\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)=\\mathbf{V}^{\\prime} \\mathbf{R V}\\) where \\(\\mathbf{V}=\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\) and \\(\\mathbf{R}=\\mathbf{T}^{\\prime} \\mathbf{A T}\\). By Theorem \\(2.1 .2\\) with \\(n \\times n\\) matrix \\(\\mathbf{B}=\\mathbf{I}_{n}\\) and \\(n \\times 1\\) vector \\(\\mathbf{b}=\\mathbf{T}^{-1} \\boldsymbol{\\mu}, \\mathbf{V} \\sim \\mathbf{N}_{n}\\left(\\mathbf{T}^{-1} \\boldsymbol{\\mu}, \\mathbf{I}_{n}\\right) .\\) By Theorem \\(3.1 .1, \\mathbf{V}^{\\prime} \\mathbf{R V} \\sim \\chi_{p}^{2}(\\lambda)\\) if and only if \\(\\mathbf{R}\\) is idempotent of rank \\(p .\\) But \\(\\mathbf{R}\\) is idempotent if and only if \\(\\left(\\mathbf{T}^{\\prime} \\mathbf{A T}\\right)\\left(\\mathbf{T}^{\\prime} \\mathbf{A T}\\right)=\\mathbf{T}^{\\prime} \\mathbf{A T}\\) or equivalently \\(\\mathbf{A} \\mathbf{\\Sigma}=\\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A} \\mathbf{\\Sigma}\\). Therefore, \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y} \\sim \\chi_{p}^{2}(\\lambda)\\) if and only if \\(\\mathbf{A} \\boldsymbol{\\Sigma}\\) is idempotent. Finally, \\(p=\\) \\(\\operatorname{rank}(\\mathbf{R})=\\operatorname{rank}\\left(\\mathbf{T}^{\\prime} \\mathbf{A T}\\right)=\\operatorname{rank}\\left(\\mathbf{A T T}^{\\prime}\\right)=\\operatorname{rank}(\\mathbf{A} \\mathbf{\\Sigma})\\) since \\(\\mathbf{T}\\) is nonsingular. Also, \\(\\lambda=\\left(\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)^{\\prime} \\mathbf{R}\\left(\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right) / 2=\\boldsymbol{\\mu}^{\\prime} \\mathbf{T}^{-1 /} \\mathbf{T}^{\\prime} \\mathbf{A T T}^{-1} \\boldsymbol{\\mu}=\\boldsymbol{\\mu}^{\\prime} \\mathbf{A} \\boldsymbol{\\mu} / 2 .\\) The proofs for \\(\\Sigma \\mathbf{A}\\) idempotent of rank \\(p\\) or \\(\\mathbf{A} \\Sigma \\mathbf{A}=\\mathbf{A}\\) of rank \\(p\\) are left to the reader. ◻ It is convenient to make an observation at this point. In most applications it is more natural to show that \\(\\mathbf{A} \\mathbf{\\Sigma}\\) is a multiple of an idempotent matrix. We therefore state the following two corollaries which are direct consequences of Theorem \\(3.1 .2\\). \\(a\\) Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathbf{N}_{n}(\\mu, \\mathbf{\\Sigma})\\) where \\(\\boldsymbol{\\Sigma}\\) is an \\(n \\times n\\) positive definite matrix of rank \\(n .\\) Then \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y} \\sim c \\chi_{p}^{2}\\left(\\lambda=\\boldsymbol{\\mu}^{\\prime} \\mathbf{A} \\boldsymbol{\\mu} /(2 c)\\right)\\) if and only if (1) \\(\\mathbf{A} \\mathbf{\\Sigma}(\\) or \\(\\mathbf{\\Sigma A})\\) is a multiple of an idempotent matrix of rank \\(p\\) where the multiple is c or (2) \\(\\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A}=c \\mathbf{A}\\) and \\(\\mathbf{A}\\) has rank \\(p\\). \\(b\\) If the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathbf{N}_{n}\\left(\\mathbf{0}, \\sigma^{2} \\mathbf{V}\\right)\\) where \\(\\mathbf{V}\\) is an \\(n \\times n\\) positive definite matrix of known constants then \\(\\mathbf{Y}^{\\prime} \\mathbf{V}^{-1} \\mathbf{Y} / \\sigma^{2} \\sim \\chi_{n}^{2}(\\lambda=0)\\). There are cases where \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) but \\(\\mathbf{A} \\boldsymbol{\\Sigma}\\) is not a multiple of an idempotent matrix. Such situations are covered by the next theorem. If the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathbf{N}_{n}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\\) where \\(\\boldsymbol{\\Sigma}\\) is a positive definite matrix of rank \\(n\\), then \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y} \\sim \\sum_{i=1}^{p} \\lambda_{i} W_{i}^{2}\\) where \\(p=\\operatorname{rank}(\\mathbf{A} \\boldsymbol{\\Sigma}) ; W_{i}^{2}\\) are independent \\(\\chi_{1}^{2}\\left(\\delta_{i}\\right)\\) random variables for \\(i=1, \\ldots, p ;\\) and \\(\\lambda_{1}, \\ldots, \\lambda_{p}\\) are the nonzero eigenvalues of \\(\\mathbf{A} \\mathbf{\\Sigma}\\). Proof. Proof. \\(\\quad\\) Let \\(\\mathbf{Z}=\\mathbf{T}^{-1}(\\mathbf{Y}-\\boldsymbol{\\mu})\\) where \\(\\mathbf{\\Sigma}=\\mathbf{T T}^{\\prime} .\\) By Theorem \\(2.1 .2\\) with \\(n \\times n\\) matrix \\(\\mathbf{B}=\\mathbf{T}^{-1}\\) and \\(n \\times 1\\) vector \\(\\mathbf{b}=-\\mathbf{T}^{-1} \\boldsymbol{\\mu}, \\mathbf{Z} \\sim \\mathrm{N}_{n}\\left(\\mathbf{0}, \\mathbf{I}_{n}\\right) .\\) Furthermore, \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y}=(\\mathbf{T Z}+\\boldsymbol{\\mu})^{\\prime} \\mathbf{A}(\\mathbf{T Z}+\\boldsymbol{\\mu})=\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)^{\\prime} \\mathbf{T}^{\\prime} \\mathbf{A T}\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)=\\) \\(\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)^{\\prime} \\boldsymbol{\\Gamma} \\mathbf{D} \\boldsymbol{\\Gamma}^{\\prime}\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right)\\) where T \\(^{\\prime} \\mathbf{A T}\\) is an \\(n \\times n\\) symmetric matrix, \\(\\boldsymbol{\\Gamma}\\) is the \\(n \\times n\\) orthogonal matrix of eigenvectors of T’AT, and D is the \\(n \\times n\\) diagonal matrix of eigenvalues of \\(\\mathbf{T}^{\\prime} \\mathbf{A T}\\) such that \\(\\mathbf{T}^{\\prime} \\mathbf{A T}=\\boldsymbol{\\Gamma D} \\boldsymbol{\\Gamma}^{\\prime} .\\) The eigenvalues of \\(\\mathbf{T}^{\\prime} \\mathbf{A T}\\) are \\(\\lambda_{1}, \\ldots, \\lambda_{p}, 0, \\ldots, 0\\) and rank \\(\\left(\\mathbf{T}^{\\prime} \\mathbf{A T}\\right)=p .\\) Let \\(\\mathbf{W}=\\left(W_{1}, \\ldots, W_{n}\\right)^{\\prime}=\\) \\(\\boldsymbol{\\Gamma}^{\\prime}\\left(\\mathbf{Z}+\\mathbf{T}^{-1} \\boldsymbol{\\mu}\\right) .\\) Therefore, \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}=\\mathbf{W}^{\\prime} \\mathbf{D W}=\\sum_{i=1}^{p} \\lambda_{i} W_{i}^{2} .\\) By Theorem \\(2.1 .2\\) with \\(n \\times n\\) matrix \\(\\mathbf{B}=\\mathbf{\\Gamma}^{\\prime}\\) and \\(n \\times 1\\) vector \\(\\mathbf{b}=\\mathbf{\\Gamma}^{\\prime} \\mathbf{T}^{-1} \\boldsymbol{\\mu}, \\mathbf{W} \\sim \\mathbf{N}_{n}\\left(\\mathbf{\\Gamma}^{\\prime} \\mathbf{T}^{-1} \\boldsymbol{\\mu}, \\mathbf{I}_{n}\\right) .\\) Therefore, \\(W_{i}^{2}\\) are independent \\(\\chi_{1}^{2}\\left(\\delta_{i} \\geq 0\\right)\\) random variables for \\(i=1, \\ldots, p\\). Furthermore, \\(p=\\operatorname{rank}\\left(\\mathbf{T}^{\\prime} \\mathbf{A T}\\right)=\\operatorname{rank}\\left(\\mathbf{A T T}^{\\prime}\\right)=\\operatorname{rank}(\\mathbf{A} \\boldsymbol{\\Sigma})\\) because \\(\\mathbf{T}\\) is nonsingular. Finally, the eigenvalues of \\(\\mathbf{T}^{\\prime} \\mathbf{A T}\\) are found by solving the polynomial equation \\[\\left|\\mathbf{I}_{n}-\\lambda \\mathbf{T}^{\\prime} \\mathbf{A T}\\right|=0 .\\] Premultiplying the above expression by \\(\\left|\\mathbf{T}^{\\prime-1}\\right|\\) and postmultiplying by \\(\\left|\\mathbf{T}^{\\prime}\\right|\\) we obtain \\[\\begin{array}{r} \\left|\\mathbf{T}^{\\prime-1}\\right|\\left(\\left|\\mathbf{I}_{n}-\\lambda \\mathbf{T}^{\\prime} \\mathbf{A} \\mathbf{T}\\right|\\right)\\left|\\mathbf{T}^{\\prime}\\right|=0 \\\\ \\left|\\mathbf{T}^{\\prime-1} \\mathbf{T}^{\\prime}-\\lambda \\mathbf{T}^{\\prime-1} \\mathbf{T}^{\\prime} \\mathbf{A T T}^{\\prime}\\right|=0 \\\\ \\left|\\mathbf{I}_{n}-\\lambda \\mathbf{A} \\boldsymbol{\\Sigma}\\right|=0 \\end{array}\\] Thus, the eigenvalues \\(\\mathbf{T}^{\\prime} \\mathbf{A T}\\) are the eigenvalues of \\(\\mathbf{A} \\boldsymbol{\\Sigma}\\). ◻ We now reexamine the distributions of a number of quadratic forms previously derived in Section 2.3. From Example 2.3.1 let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}\\) \\(\\left(\\alpha \\mathbf{1}_{n}, \\sigma^{2} \\mathbf{I}_{n}\\right) .\\) By Corollary 3.1.2(a), \\(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}=\\mathbf{Y}^{\\prime}\\left(\\mathbf{I}_{n}-\\frac{1}{n} \\mathbf{J}_{n}\\right) \\mathbf{Y} \\sim \\sigma^{2} \\chi_{n-1}^{2}\\) \\((\\lambda=0)\\) since \\[\\lambda=\\left(\\alpha \\mathbf{1}_{n}\\right)^{\\prime}\\left(\\mathbf{I}_{n}-\\frac{1}{n} \\mathbf{J}_{n}\\right)\\left(\\alpha \\mathbf{1}_{n}\\right) /\\left(2 \\sigma^{2}\\right)=0\\] and \\[\\left(\\mathbf{I}_{n}-\\frac{1}{n} \\mathbf{J}_{n}\\right)\\left(\\sigma^{2} \\mathbf{I}_{n}\\right)=\\sigma^{2}\\left(\\mathbf{I}_{n}-\\frac{1}{n} \\mathbf{J}_{n}\\right)\\] which is a multiple of an idempotent matrix of rank \\(n-1\\). Furthermore, let \\(n(\\bar{Y}-\\) \\(\\alpha)^{2}=n\\left[(1 / n) \\mathbf{1}_{n}^{\\prime}\\left(\\mathbf{Y}-\\alpha \\mathbf{1}_{n}\\right)\\right]^{\\prime}\\left[(1 / n) \\mathbf{1}_{n}^{\\prime}\\left(\\mathbf{Y}-\\alpha \\mathbf{1}_{n}\\right)\\right]=\\left(\\mathbf{Y}-\\alpha \\mathbf{1}_{n}\\right)^{\\prime}\\left(\\frac{1}{n} \\mathbf{J}_{n}\\right)\\left(\\mathbf{Y}-\\alpha \\mathbf{1}_{n}\\right) .\\) By Theorem 2.1.2 with \\(n \\times n\\) matrix \\(\\mathbf{B}=\\mathbf{I}_{n}\\) and \\(n \\times 1\\) vector \\(\\left(\\mathbf{b}=-\\alpha \\mathbf{1}_{n}, \\mathbf{Y}-\\alpha \\mathbf{1}_{n} \\sim\\right.\\) \\(\\mathrm{N}_{n}\\left(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{n}\\right) .\\) Therefore, by Corollary 3.1.2(a), \\(n(\\bar{Y}-\\alpha)^{2} \\sim \\sigma^{2} \\chi_{1}^{2}(\\lambda=0)\\) since \\[\\lambda=\\left(\\mathbf{0}_{n \\times 1}\\right)^{\\prime}\\left(\\frac{1}{n} \\mathbf{J}_{n}\\right)\\left(\\mathbf{0}_{n \\times 1}\\right) /\\left(2 \\sigma^{2}\\right)=0\\] and \\[\\left(\\frac{1}{n} \\mathbf{J}_{n}\\right)\\left(\\sigma^{2} \\mathbf{I}_{n}\\right)=\\sigma^{2}\\left(\\frac{1}{n} \\mathbf{J}_{n}\\right)\\] which is a multiple of an idempotent matrix of rank \\(1 .\\) Consider the two-way cross classification from Example 2.3.2 where the st \\(\\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{11}, \\ldots, Y_{1 t}, \\ldots, Y_{s 1}, \\ldots, Y_{s t}\\right)^{\\prime} \\sim\\) \\(\\mathbf{N}_{s t}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), \\boldsymbol{\\mu}=\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\), and \\(\\boldsymbol{\\Sigma}=\\sigma_{S}^{2} \\mathbf{I}_{s} \\otimes \\mathbf{J}_{t}+\\sigma_{T}^{2} \\mathbf{J}_{s} \\otimes \\mathbf{I}_{t}+\\sigma_{S T}^{2} \\mathbf{I}_{s} \\otimes \\mathbf{I}_{t} .\\) The sum of squares due to the random factor \\(S\\) is \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}\\) where \\(\\mathbf{A}_{2}=\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes \\frac{1}{t} \\mathbf{J}_{t} .\\) By Corollary 3.1.2(a), Y’A \\(\\mathbf{A}_{2} \\mathbf{Y} \\sim\\left(\\sigma_{S T}^{2}+t \\sigma_{S}^{2}\\right) \\chi_{s-1}^{2}\\left(\\lambda_{2}=0\\right)\\) since \\(\\mathbf{A}_{2} \\Sigma=t \\sigma_{S}^{2}\\left[\\left(\\mathbf{I}_{s}-\\right.\\right.\\) \\(\\left.\\left.\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes \\frac{1}{t} \\mathbf{J}_{t}\\right]+\\sigma_{S T}^{2}\\left[\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes \\frac{1}{t} \\mathbf{J}_{t}\\right]=\\left(\\sigma_{S T}^{2}+t \\sigma_{s}^{2}\\right) \\mathbf{A}_{2}\\) \\[\\lambda_{2}=\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right)^{\\prime}\\left[\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes \\frac{1}{t} \\mathbf{J}_{t}\\right]\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right)=0\\] and \\(\\mathbf{A}_{2}\\) is an idempotent matrix of rank \\(s-1\\). The sum of squares due to the mean is \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{1} \\mathbf{Y}\\) where \\(\\mathbf{A}_{1}=\\frac{1}{s} \\mathbf{J}_{s} \\otimes \\frac{1}{t} \\mathbf{J}_{t} .\\) By Corollary \\(3.1 .2(\\mathrm{a}), \\mathbf{Y}^{\\prime} \\mathbf{A}_{\\mathbf{1}} \\mathbf{Y} \\sim\\left(t \\sigma_{S}^{2}+s \\sigma_{T}^{2}+\\right.\\) \\(\\left.\\sigma_{S T}^{2}\\right) \\chi_{1}^{2}\\left(\\lambda_{1}\\right)\\) since \\(A_{1} \\Sigma^{s}=\\left(t \\sigma_{S}^{2}+s \\sigma_{T}^{2}-\\sigma_{S T}^{2}\\right) \\mathbf{A}_{1}, \\mathbf{A}_{1}\\) is an idempotent matrix of rank 1 , and \\[\\begin{aligned} \\lambda_{1} &amp;=\\left\\{1 /\\left[2\\left(t \\sigma_{s}^{2}+s \\sigma_{T}^{2}+\\sigma_{S T}^{2}\\right)\\right]\\right\\}\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right)^{\\prime}\\left[\\frac{1}{s} \\mathbf{J}_{s} \\otimes \\frac{1}{t} \\mathbf{J}_{t}\\right]\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right) \\\\ &amp;=s t \\alpha^{2} /\\left[2\\left(t \\sigma_{S}^{2}+s \\sigma_{T}^{2}+\\sigma_{S T}^{2}\\right)\\right] \\end{aligned}\\] The distributions of the other quadratic forms in Example \\(2.3 .2\\) can be derived in a similar manner and are left to the reader. 4.2 INDEPENDENCE The independence of two quadratic forms is examined in the next theorem. Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be \\(n \\times n\\) constant matrices. Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim N_{n}(\\mu, \\Sigma)\\). The quadratic forms \\(\\mathbf{Y}^{\\prime} \\mathbf{A Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{B} \\mathbf{Y}\\) are independent if and only if \\(\\mathbf{A} \\boldsymbol{\\Sigma} \\mathbf{B}=\\mathbf{0}\\) (or \\(\\mathbf{B} \\Sigma \\mathbf{A}=\\mathbf{0}\\) ). Proof. Proof. The matrices \\(\\mathbf{A}, \\Sigma\\), and \\(\\mathbf{B}\\) are symmetric. Therefore, \\(\\mathbf{A} \\boldsymbol{\\Sigma} \\mathbf{B}=\\mathbf{0}\\) is equivalent to \\(\\mathbf{B} \\Sigma A=\\mathbf{0}\\). Assume \\(\\mathbf{A} \\Sigma \\mathbf{B}=\\mathbf{0}\\). Since \\(\\boldsymbol{\\Sigma}\\) is positive definite, by Theorem 1.1.5, there exists an \\(n \\times n\\) nonsingular matrix \\(\\mathbf{S}\\) such that \\(\\mathbf{S} \\Sigma \\mathbf{S}^{\\prime}=\\mathbf{I}_{n}\\). Then \\(\\mathbf{Z}=\\mathbf{S Y} \\sim \\mathbf{N}_{n}\\left(\\mathbf{S} \\boldsymbol{\\mu}, \\mathbf{I}_{n}\\right) .\\) Let \\(\\mathbf{G}=\\left(\\mathbf{S}^{-1}\\right)^{\\prime} \\mathbf{A S}^{-1}\\) and \\(\\mathbf{H}=\\left(\\mathbf{S}^{-1}\\right)^{\\prime} \\mathbf{B S}^{-1} .\\) Therefore, \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}=\\mathbf{Z}^{\\prime} \\mathbf{G Z}, \\mathbf{Y}^{\\prime} \\mathbf{B} \\mathbf{Y}=\\mathbf{Z}^{\\prime} \\mathbf{H Z}\\), and \\(\\mathbf{A} \\mathbf{\\Sigma B}=\\mathbf{S}^{\\prime} \\mathbf{G S} \\mathbf{\\Sigma S}^{\\prime} \\mathbf{H S}=\\mathbf{S}^{\\prime} \\mathbf{G H S} .\\) Thus, the statement \\(\\mathbf{A} \\mathbf{\\Sigma B}=\\mathbf{0}\\) implies \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{B Y}\\) are independent and the statement \\(\\mathbf{G H}=\\mathbf{0}\\), implies \\(\\mathbf{Z}^{\\prime} \\mathbf{G Z}\\) and \\(\\mathbf{Z}^{\\prime} \\mathbf{H Z}\\) are independent, are equivalent. Since \\(\\mathbf{G}\\) is symmetric, there exists an orthogonal matrix \\(\\mathbf{P}\\) such that \\(\\mathbf{G}=\\mathbf{P}^{\\prime} \\mathbf{D} \\mathbf{P}\\), where \\(a=\\operatorname{rank}(\\mathbf{A})=\\operatorname{rank}(\\mathbf{G})\\) and \\(\\mathbf{D}\\) is a diagonal matrix with \\(a\\) nonzero diagonal elements. Without loss of generality, assume that \\[\\mathbf{D}=\\left[\\begin{array}{cc} \\mathbf{D}_{a} &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{0} \\end{array}\\right]\\] where \\(\\mathbf{D}_{a}\\) is the \\(a \\times a\\) diagonal matrix containing the nonzero elements of \\(\\mathbf{D}\\). Let \\(\\mathbf{X}=\\mathbf{P Z} \\sim \\mathrm{N}_{n}\\left(\\mathbf{P S} \\boldsymbol{\\mu}, \\mathbf{I}_{n}\\right)\\) and partition \\(\\mathbf{X}\\) as \\(\\left[\\mathbf{X}_{1}^{\\prime}, \\mathbf{X}_{2}^{\\prime}\\right]^{\\prime}\\), where \\(\\mathbf{X}_{1}\\) is \\(a \\times 1\\). Note that \\(\\mathbf{X}_{1}\\) and \\(\\mathbf{X}_{2}\\) are independent. Then \\(\\mathbf{Z}^{\\prime} \\mathbf{G Z}=\\mathbf{Z}^{\\prime} \\mathbf{P}^{\\prime} \\mathbf{D P Z}=\\mathbf{X}^{\\prime} \\mathbf{D} \\mathbf{X}=\\mathbf{X}_{1}^{\\prime} \\mathbf{D}_{a} \\mathbf{X}_{1}\\) and \\(\\mathbf{Z}^{\\prime} \\mathbf{H Z}=\\mathbf{X}^{\\prime} \\mathbf{P H} \\mathbf{P}^{\\prime} \\mathbf{X}=\\mathbf{X}^{\\prime} \\mathbf{C X}\\) where the symmetric matrix \\(\\mathbf{C}=\\mathbf{P H} \\mathbf{P}^{\\prime} .\\) If \\(\\mathbf{G H}=\\mathbf{0}\\) then \\(\\mathbf{P}^{\\prime} \\mathbf{D P P}^{\\prime} \\mathbf{C P}=\\mathbf{0}\\), which implies \\(\\mathbf{D C}=\\mathbf{0} .\\) Partitioning \\(\\mathbf{C}\\) to conform with \\(\\mathbf{D}\\), \\[\\mathbf{0}=\\mathbf{D} \\mathbf{C}=\\left[\\begin{array}{cc} \\mathbf{D}_{a} &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{0} \\end{array}\\right]\\left[\\begin{array}{ll} \\mathbf{C}_{11} &amp; \\mathbf{C}_{12} \\\\ \\mathbf{C}_{12}^{\\prime} &amp; \\mathbf{C}_{22} \\end{array}\\right]=\\left[\\begin{array}{cc} \\mathbf{D}_{a} \\mathbf{C}_{11} &amp; \\mathbf{D}_{a} \\mathbf{C}_{12} \\\\ \\mathbf{0} &amp; \\mathbf{0} \\end{array}\\right]\\] which implies \\(\\mathbf{C}_{11}=\\mathbf{0}\\) and \\(\\mathbf{C}_{12}=\\mathbf{0}\\). Therefore, \\[\\mathbf{C}=\\left[\\begin{array}{cc} \\mathbf{0} &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\mathbf{C}_{22} \\end{array}\\right]\\] which implies \\(\\mathbf{X}^{\\prime} \\mathbf{C X}=\\mathbf{X}_{2}^{\\prime} \\mathbf{C}_{22} \\mathbf{X}_{2}\\), which is independent of \\(\\mathbf{X}_{1}^{\\prime} \\mathbf{D}_{a} \\mathbf{X}_{1} .\\) Therefore, \\(\\mathbf{Z}^{\\prime} \\mathbf{G Z}\\) and \\(\\mathbf{Z}^{\\prime} \\mathbf{H Z}\\) are independent. The proof of the converse statement is supplied by Searle (1971). ◻ The following theorem considers the independence of a quadratic form and linear combinations of a normally distributed random vector. Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be \\(n \\times n\\) and \\(m \\times n\\) constant matrices, respectively. Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}(\\boldsymbol{\\mu}, \\Sigma)\\). The quadratic form \\(\\mathbf{Y}^{\\prime} \\mathbf{A} \\mathbf{Y}\\) and the set of linear combinations \\(\\mathbf{B Y}\\) are independent if and only if \\(\\mathbf{B} \\mathbf{\\Sigma} \\mathbf{A}=\\mathbf{0}(\\) or \\(\\left.\\mathbf{A} \\mathbf{\\Sigma B}^{\\prime}=\\mathbf{0}\\right)\\) Proof. Proof. The \"if\" portion can be proven by the same method used in the proof of Theorem 3.2.1. The proof of the converse statement is supplied by Searle (1971). ◻ In the following examples the independence of certain quadratic forms and linear combinations is examined. Consider the one-way classification described in Examples \\(1.2 .10\\) and 2.1.4. The sum of squares due to the fixed factor is \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}\\) where \\(\\mathbf{A}_{2}=\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right) \\otimes \\frac{1}{r} \\mathbf{J}_{r}\\) is an idempotent matrix of rank \\(t-1 .\\) Furthermore, \\(\\mathbf{A}_{2} \\Sigma=\\left[\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right) \\otimes \\frac{1}{r} \\mathbf{J}_{r}\\right]\\left[\\sigma^{2} \\mathbf{I}_{t} \\otimes \\mathbf{I}_{r}\\right]=\\sigma^{2} \\mathbf{A}_{2}\\). The sum of squares due to the nested replicates is \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y}\\) where \\(\\mathbf{A}_{3}=\\mathbf{I}_{t} \\otimes\\left(\\mathbf{I}_{r}-\\frac{1}{r} \\mathbf{J}_{r}\\right)\\) is an idempotent matrix of \\(\\operatorname{rank} t(r-1) .\\) Likewise, \\(\\mathbf{A}_{3} \\Sigma=\\left[\\mathbf{I}_{t} \\otimes\\left(\\mathbf{I}_{r}-\\frac{1}{r} \\mathbf{J}_{r}\\right)\\right]\\left[\\sigma^{2} \\mathbf{I}_{t} \\otimes \\mathbf{I}_{r}\\right]=\\sigma^{2} \\mathbf{A}_{3} .\\) Therefore, by Corollary 3.1.2(a), \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y} \\sim \\sigma^{2} \\chi_{t-1}^{2}\\left(\\lambda_{2}\\right)\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y} \\sim \\sigma^{2} \\chi_{t(r-1)}^{2}\\left(\\lambda_{3}\\right)\\) where \\(\\lambda_{2}=\\left[\\left(\\mu_{1}, \\ldots, \\mu_{t}\\right)^{\\prime} \\otimes \\mathbf{1}_{r}\\right]^{\\prime}\\left[\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right) \\otimes \\frac{1}{r} \\mathbf{J}_{r}\\right]\\left[\\left(\\mu_{1}, \\ldots, \\mu_{t}\\right)^{\\prime} \\otimes \\mathbf{1}_{r}\\right] /\\left(2 \\sigma^{2}\\right)=\\) \\(r \\sum_{i=1}^{t}\\left(\\mu_{i}-\\bar{\\mu} .\\right)^{2} /\\left(2 \\sigma^{2}\\right)\\) with \\(\\bar{\\mu} .=\\sum_{i=1}^{t} \\mu_{i} / t\\) and \\(\\lambda_{3}=\\left[\\left(\\mu_{1}, \\ldots, \\mu_{t}\\right)^{\\prime} \\otimes\\right.\\) \\(\\left.\\mathbf{1}_{r}\\right]^{\\prime}\\left[\\mathbf{I}_{t} \\otimes\\left(\\mathbf{I}_{r}-\\frac{1}{\\mathbf{J}}_{r}\\right)\\right]\\left[\\left(\\mu_{1}, \\ldots, \\mu_{t}\\right)^{\\prime} \\otimes \\mathbf{1}_{r}\\right] /\\left(2 \\sigma^{2}\\right)=0 .\\) Finally, by Theo\\(\\left.\\mathbf{1}_{r}\\right]^{\\prime}\\left[\\mathbf{I}_{t} \\otimes\\left(\\mathbf{I}_{r}-\\frac{1}{r} \\mathbf{J}_{r}\\right)\\right]\\left[\\left(\\mu_{1}, \\ldots, \\mu_{t}\\right) \\otimes \\mathbf{1}_{r}\\right] /\\left(2 \\sigma^{2}\\right)=0 .\\) Finally, by Theo rem 3.2.1, \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y}\\) are independent since \\(\\mathbf{A}_{2} \\Sigma \\mathbf{A}_{3}=\\sigma^{2} \\mathbf{A}_{2} \\mathbf{A}_{3}=\\) rem 3.2.1, Y’A \\(_{2} \\mathbf{Y}\\) and \\(\\mathbf{Y} \\mathbf{A}_{3} \\mathbf{Y}\\) are independent \\(\\sigma^{2}\\left[\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right) \\otimes \\frac{1}{r} \\mathbf{J}_{r}\\right]\\left[\\mathbf{I}_{t} \\otimes\\left(\\mathbf{I}_{r}-\\frac{1}{r} \\mathbf{J}_{r}\\right)\\right]=\\mathbf{0}_{t r \\times t r}\\) Reconsider Example 2.3.1 where \\(\\mathbf{Y}=\\left(Y_{1}, \\ldots, Y_{n}\\right)^{\\prime} \\sim \\mathrm{N}_{n}\\left(\\alpha \\mathbf{1}_{n},\\right.\\), \\(\\left.\\sigma^{2} \\mathbf{I}_{n}\\right), U=\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} / \\sigma^{2}=\\mathbf{y}^{\\prime}\\left[\\left(1 / \\sigma^{2}\\right)\\left(\\mathbf{I}_{n}-\\frac{1}{n} \\mathbf{J}_{n}\\right)\\right] \\mathbf{Y}\\) and \\(\\bar{Y}=(1 / n) \\mathbf{1}_{n}^{\\prime} \\mathbf{Y} .\\) By Theorem 3.2.2, \\(\\bar{Y}\\) and \\(U\\) are independent since \\((1 / n) \\mathbf{1}_{n}^{\\prime}\\left[\\sigma^{2} \\mathbf{I}_{n}\\right]\\left[\\left(1 / \\sigma^{2}\\right)\\left(\\mathbf{I}_{n}-\\right.\\right.\\) \\(\\left.\\left.\\frac{1}{n} \\mathbf{J}_{n}\\right)\\right]=\\mathbf{0}_{1 \\times n} .\\) 4.3 THE \\(\\boldsymbol{t}\\) AND \\(\\boldsymbol{F}\\) DISTRIBUTIONS The normal and chi-square distributions were discussed at length in the previous sections. We now examine the distributions of certain functions of chi-square and normal random variables. Noncentral \\(t\\) Random Variable: Let the random variable \\(Y \\sim\\) \\(\\mathrm{N}_{1}\\left(\\alpha, \\sigma^{2}\\right)\\) and the random variable \\(U \\sim \\chi_{n}^{2}(0) .\\) If \\(Y\\) and \\(U\\) are independent, then the random variable \\(T=(Y / \\sigma) / \\sqrt{U / n}\\) is distributed as a noncentral \\(t\\) random variable with \\(n\\) degrees of freedom and noncentrality parameter \\(\\lambda=\\alpha^{2} / 2\\). Denote this noncentral \\(t\\) random variable as \\(t_{n}(\\lambda)\\). Noncentral F Random Variable: Let the random variable \\(U_{1} \\sim\\) \\(\\chi_{n_{1}}^{2}(\\lambda)\\) and the random variable \\(U_{2} \\sim \\chi_{n_{2}}^{2}(0) .\\) If \\(U_{1}\\) and \\(U_{2}\\) are independent, then the random variable \\(F=\\left(U_{1} / n_{1}\\right) /\\left(U_{2} / n_{2}\\right)\\) is distributed as a noncentral \\(F\\) random variable with \\(n_{1}\\) and \\(n_{2}\\) degrees of freedom and noncentrality parameter \\(\\lambda\\). Denote this noncentral \\(F\\) random variable as \\(F_{n_{1}, n_{2}}(\\lambda)\\). A \\(t\\) random variable with \\(n\\) degrees of freedom and a noncentrality parameter equal to zero \\[i.e., $t_{n}(\\lambda=0)$ \\] has a central \\(t\\) distribution. Likewise, an \\(F\\) random variable with \\(n_{1}\\) and \\(n_{2}\\) degrees of freedom and a noncentrality parameter equal to zero \\[i.e., $F_{n_{1}, n_{2}}(\\lambda=0)$ \\] has a central \\(F\\) distribution. In recent years Smith and Lewis \\((1980,1982)\\), Pavur and Lewis (1983), Scariano, Neill, and Davenport (1984) and Scariano and Davenport (1984) have developed the theory of the corrected \\(F\\) random variable. The definition of the corrected \\(F\\) random variable is given next. Noncentral Corrected F Random Variable: Let the random variable \\(U_{1} \\sim c_{1} \\chi_{n_{1}}^{2}(\\lambda)\\) and the random variable \\(U_{2} \\sim c_{2} \\chi_{n_{2}}^{2}(0) .\\) If \\(U_{1}\\) and \\(U_{2}\\) are independent, then the random variable \\(F_{c}=\\left(c_{2} / c_{1}\\right)\\left[\\left(U_{1} / n_{1}\\right) /\\left(U_{2} / n_{2}\\right)\\right] \\sim\\) \\(F_{n_{1}, n_{2}}(\\lambda)\\) is called a corrected \\(F\\) random variable where the ratio \\(c_{2} / c_{1}\\) is the correction factor. In practice, we often encounter independent random variables \\(U_{1}\\) and \\(U_{2}\\), which are distributed as multiples of chi-square random variables \\(\\left(U_{2}\\right.\\) being a multiple of a central chi square). The random variable \\(F=\\left(U_{1} / n_{1}\\right) /\\left(U_{2} / n_{2}\\right)\\) in this case will be distributed as a noncentral \\(F\\) random variable if and only if \\(c_{1}=c_{2}\\) (i.e., \\(c_{2} / c_{1}=1\\) ). Generally, \\(c_{1}\\) and \\(c_{2}\\) will be linear combinations of unknown variance parameters. In the following examples a number of central and noncentral \\(t\\) and \\(F\\) random variables are derived. Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{1}, \\ldots, Y_{n}\\right)^{\\prime} \\sim \\mathrm{N}_{n}\\left(\\alpha \\mathbf{1}_{n}, \\sigma^{2} \\mathbf{I}_{n}\\right)\\) By Example 2.1.1, \\(\\bar{Y} \\sim \\mathrm{N}_{1}\\left(\\alpha, \\sigma^{2} / n\\right) .\\) By Example 3.1.1, \\(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}=\\) \\(\\mathbf{Y}^{\\prime}\\left[\\left(\\mathbf{I}_{n}-\\frac{1}{n} \\mathbf{J}_{n}\\right)\\right] \\mathbf{Y} \\sim \\sigma^{2} \\chi_{n-1}^{2}(0) .\\) By Example 3.2.2. \\(\\bar{Y}\\) and \\(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}\\) are independent. Therefore, \\[T=\\sqrt{n} \\bar{Y} / S=[\\bar{Y} /(\\sigma / \\sqrt{n})] / \\sqrt{S^{2} / \\sigma^{2}} \\sim t_{n-1}(\\lambda)\\] where \\(S^{2}=\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} /(n-1)\\) and \\(\\lambda=\\alpha^{2} / 2\\). Consider the one-way classification described in Example 3.2.1. It was shown that the sum of squares due to the fixed factor \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}=\\mathbf{Y}^{\\prime}\\left[\\left(\\mathbf{I}_{t}-\\right.\\right.\\) \\(\\left.\\left.\\frac{1}{t} \\mathbf{J}_{t}\\right) \\otimes \\frac{1}{r} \\mathbf{J}_{r}\\right] \\mathbf{Y} \\sim \\sigma^{2} \\chi_{t-1}^{2}\\left(\\lambda_{2}\\right)\\) and the sum of squares due to the nested replicates \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y}=\\mathbf{Y}^{\\prime}\\left[\\mathbf{I}_{t} \\otimes\\left(\\mathbf{I}_{r}-\\frac{1}{r} \\mathbf{J}_{r}\\right)\\right] \\mathbf{Y} \\sim \\sigma^{2} \\chi_{t(r-1)}^{2}(0) .\\) Furthermore, \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y}\\) are independent. Therefore, the statistic \\[F^{*}=\\frac{\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y} /(t-1)}{\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y} /[t(r-1)]} \\sim F_{t-1, t(r-1)}\\left(\\lambda_{2}\\right)\\] where \\(\\lambda_{2}=r \\sum_{i=1}^{t}\\left(\\mu_{i}-\\bar{\\mu} \\cdot\\right)^{2} /\\left(2 \\sigma^{2}\\right) .\\) The hypothesis \\(\\mathrm{H}_{0}: \\lambda_{2}=0\\) versus \\(\\mathrm{H}_{1}: \\lambda&gt;0\\) is equivalent to the hypothesis \\(\\mathrm{H}_{0}: \\mu_{1}=\\mu_{2}=\\cdots=\\mu_{t}\\) versus \\(\\mathrm{H}_{1}\\) : the \\(\\mu_{i}\\) ’s are not all equal. Thus, under \\(\\mathrm{H}_{0}\\), the statistic \\(F^{*}\\) has a central \\(F\\) distribution with \\(t-1\\) and \\(t(r-1)\\) degrees of freedom. A \\(\\gamma\\) level rejection region for the hypothesis \\(\\mathrm{H}_{0}\\) versus \\(\\mathrm{H}_{1}\\) is as follows: Reject \\(\\mathrm{H}_{0}\\) if \\(F^{*}&gt;F_{t-1, t(r-1)}^{\\gamma}\\) where \\(F_{t-1, t(r-1)}^{\\gamma}\\) is the \\(100(1-\\gamma)^{\\text {th }}\\) percentile point of a central \\(F\\) distribution with \\(t-1\\) and \\(t(r-1)\\) degrees of freedom. Consider the two-way cross classification described in Example 3.1.2. The sums of squares due to the random factor \\(S\\) and due to the random interaction \\(S T\\) are given by \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{4} \\mathbf{Y}\\), respectively. It was shown that \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}=\\mathbf{Y}^{\\prime}\\left[\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes \\frac{1}{t} \\mathbf{J}_{t}\\right] \\mathbf{Y} \\sim\\left(\\sigma_{S T}^{2}+t \\sigma_{S}^{2}\\right) \\chi_{s-1}^{2}(0) .\\) Furthermore, \\(\\mathbf{A}_{4} \\boldsymbol{\\Sigma}=\\) \\(\\left[\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right)\\right]\\left[\\sigma_{S}^{2} \\mathbf{I}_{s} \\otimes \\mathbf{J}_{t}+\\sigma_{T}^{2} \\mathbf{J}_{s} \\otimes \\mathbf{I}_{t}+\\sigma_{S T}^{2} \\mathbf{I}_{s} \\otimes \\mathbf{I}_{t}\\right]=\\sigma_{S T}^{2} \\mathbf{A}_{4}\\) where \\(\\mathbf{A}_{4}\\) is an idempotent matrix of rank \\((s-1)(t-1)\\). Therefore, by Corollary \\(3.1 .2(\\mathrm{a}), \\mathbf{Y}^{\\prime} \\mathbf{A}_{4} \\mathbf{Y}=\\mathbf{Y}^{\\prime}\\left[\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right)\\right] \\mathbf{Y} \\sim \\sigma_{S T}^{2} \\chi_{(s-1)(t-1)}^{2}\\left(\\lambda_{4}\\right)\\) where \\(\\lambda_{4}=\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right)^{\\prime}\\left[\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right)\\right]\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right) /\\left(2 \\sigma_{S T}^{2}\\right)=0 .\\) Finally, \\(\\mathbf{A}_{2} \\boldsymbol{\\Sigma} \\mathbf{A}_{4}=\\left(\\sigma_{S T}^{2}+t \\sigma_{S}^{2}\\right) \\mathbf{A}_{2} \\mathbf{A}_{4}=\\mathbf{0}_{s t \\times s t} .\\) Therefore, by Theorem 3.2.1, \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{4} \\mathbf{Y}\\) are independent. By Definition \\(3.3 .3\\), the statistic \\[F^{*}=\\frac{\\mathbf{Y}^{\\prime} \\mathbf{A}_{2} \\mathbf{Y} /(s-1)}{\\mathbf{Y}^{\\prime} \\mathbf{A}_{4} \\mathbf{Y} /[(s-1)(t-1)]} \\sim \\frac{\\left(\\sigma_{S T}^{2}+t \\sigma_{S}^{2}\\right)}{\\sigma_{S T}^{2}} F_{s-1,(s-1)(t-1)}(0)\\] Under the hypothesis \\(\\mathrm{H}_{0}: \\sigma_{S}^{2}=0\\), the statistic \\(F^{*}\\) has a central \\(F\\) distribution with \\(s-1\\) and \\((s-1)(t-1)\\) degrees of freedom. A \\(\\gamma\\) level rejection region for the hypothesis \\(\\mathrm{H}_{0}: \\sigma_{S}^{2}=0\\) versus \\(\\mathrm{H}_{1}: \\sigma_{S}^{2}&gt;0\\) follows; Reject \\(\\mathrm{H}_{0}\\) if \\(F^{*}&gt;F_{s-1,(s-1)(t-1)}^{\\gamma} .\\) 4.4 BHAT’S LEMMA The following lemma by Bhat (1962) is applicable in many ANOVA and regression problems. The lemma provides necessary and sufficient conditions for sums of squares to be distributed as multiples of independent chi-square random variables. Let \\(k\\) and \\(n\\) denote fixed positive integers such that \\(1 \\leq k \\leq n\\). Suppose \\(\\mathbf{I}_{n}=\\sum_{i=1}^{k} \\mathbf{A}_{i}\\), where each \\(\\mathbf{A}_{i}\\) is an \\(n \\times n\\) symmetric matrix of rank \\(n_{i}\\) with \\(\\sum_{i=1}^{k} n_{i}=n .\\) If the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}(\\mu, \\mathbf{\\Sigma})\\) and the sum of squares \\(S_{i}^{2}=\\mathbf{Y}^{\\prime} \\mathbf{A}_{i} \\mathbf{Y}\\) for \\(i=1, \\ldots, k\\), then \\(S_{i}^{2} \\sim c_{i} \\chi_{n_{i}}^{2}\\left(\\lambda_{i}=\\mu^{\\prime} \\mathbf{A}_{i} \\mu /\\left(2 c_{i}\\right)\\right)\\) and if and only if \\(\\Sigma=\\sum_{i=1}^{k} c_{i} \\mathbf{A}_{i}\\) where \\(c_{i}&gt;0\\). Proof. Proof. This proof is due to Scariano et al. (1984). Assume that the quadratic forms \\(S_{i}^{2}\\) satisfy (a) and (b) given in Lemma 3.4.1. By Theorems \\(3.1 .2\\) and 3.2.1, (i) the matrices \\(\\left(1 / c_{i}\\right) \\mathbf{A}_{i} \\Sigma\\) are idempotent for \\(i=1, \\ldots, k\\) and (ii) \\(\\mathbf{A}_{i} \\mathbf{\\Sigma A}_{j}=\\mathbf{0}_{n \\times n}\\) for \\(i \\neq j, i, j=1, \\ldots, k\\). Furthermore, by Theorem 1.1.7, \\(\\mathbf{A}_{i}=\\mathbf{A}_{i}^{2}\\) and \\(\\mathbf{A}_{i} \\mathbf{A}_{j}=\\mathbf{0}_{n \\times n}\\) for \\(i \\neq j, i, j=1, \\ldots, k .\\) But (i) and (ii) imply that \\(\\sum_{i=1}^{k}\\left(1 / c_{i}\\right) \\mathbf{A}_{i} \\Sigma\\) is idempotent of rank \\(n\\) and thus equal to \\(\\mathbf{I}_{n} .\\) Hence, \\(\\Sigma=\\left[\\sum_{i=1}^{k}\\left(1 / c_{i}\\right) \\mathbf{A}_{i}\\right]^{-1}:=\\) \\(\\sum_{i=1}^{k} c_{i} \\mathbf{A}_{i} .\\) Conversely, assume \\(\\Sigma=\\sum_{i=1}^{k} c_{i} \\mathbf{A}_{i} .\\) But \\(\\mathbf{A}_{i}=\\mathbf{A}_{i}^{2}\\) and \\(\\mathbf{A}_{i} \\mathbf{A}_{j}=\\mathbf{0}_{n \\times n}\\), so (i) and (ii) hold. Therefore, by Theorems \\(3.1 .2\\) and \\(3.2 .1\\), (a) and (b) hold. ◻ In the next example, Bhat’s lemma is applied to the two-way cross classification described in Example 2.3.2 From Example 2.3.2, \\(\\Sigma=\\left[\\sigma_{S}^{2} \\mathbf{I}_{s} \\otimes \\mathbf{J}_{t}+\\sigma_{T}^{2} \\mathbf{J}_{s} \\otimes \\mathbf{I}_{t}+\\sigma_{S T}^{2} \\mathbf{I}_{s} \\otimes\\right.\\) \\(\\left.\\mathbf{I}_{t}\\right], \\mathbf{A}_{1}=\\frac{1}{s} \\mathbf{J}_{s} \\otimes \\frac{1}{t} \\mathbf{J}_{t}, \\mathbf{A}_{2}=\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes \\frac{1}{t} \\mathbf{J}_{t}, \\mathbf{A}_{3}=\\frac{1}{s} \\mathbf{J}_{s} \\otimes\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right), \\mathbf{A}_{4}=\\) \\(\\left(\\mathbf{I}_{s}-\\frac{1}{s} \\mathbf{J}_{s}\\right) \\otimes\\left(\\mathbf{I}_{t}-\\frac{1}{t} \\mathbf{J}_{t}\\right), s t=\\sum_{m=1}^{4} \\operatorname{rank}\\left(\\mathbf{A}_{m}\\right)=1+(s-1)+(t-1)+(s-1)(t-1)\\), and \\(\\mathbf{I}_{s} \\otimes \\mathbf{I}_{t}=\\sum_{m=1}^{4} \\mathbf{A}_{m} .\\) Furthermore, \\(\\mathbf{A}_{m} \\boldsymbol{\\Sigma}=c_{m} \\mathbf{A}_{m}\\) for \\(m=1, \\ldots, 4\\) where \\(c_{1}=\\sigma_{S T}^{2}+t \\sigma_{S}^{2}+s \\sigma_{T}^{2}, c_{2}=\\sigma_{S T}^{2}+t \\sigma_{S}^{2}, c_{3}=\\sigma_{S T}^{2}+s \\sigma_{T}^{2}\\), and \\(c_{4}=\\sigma_{S T}^{2} .\\) Therefore, \\(\\boldsymbol{\\Sigma}=\\left(\\sum_{m=1}^{4} \\mathbf{A}_{m}\\right) \\boldsymbol{\\Sigma}=\\sum_{m=1}^{4}\\left(\\mathbf{A}_{m} \\boldsymbol{\\Sigma}\\right)=\\sum_{m=1}^{4} c_{m} \\mathbf{A}_{m}\\). Thus, by Bhat’s lemma, the quadratic forms \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{m} \\mathbf{Y}\\) are distributed as independent \\(c_{m} \\chi_{\\operatorname{rank}\\left(\\mathbf{A}_{m}\\right)}^{2}\\left\\{\\lambda_{m}=\\left(\\alpha \\mathbf{1}_{s} \\otimes\\right.\\right.\\) \\(\\left.\\left.\\mathbf{1}_{t}\\right)^{\\prime} \\mathbf{A}_{m}\\left(\\alpha \\mathbf{1}_{s} \\otimes \\mathbf{1}_{t}\\right) /\\left(2 c_{m}\\right)\\right\\}\\) for \\(m=1, \\ldots, 4 .\\) 4.5 EXERCISES Use Corollary \\(3.1 .2\\) (a) to find the distribution of \\(\\sum_{i=1}^{n} w_{i} Y_{i}^{2}\\) from Exercise \\(3 \\mathrm{~b}\\) in Chapter \\(2 .\\) Use Corollary 3.1.2(a) to find the distribution of Y’AY from Exercise \\(4 \\mathrm{c}\\) in Chapter 2 . Consider the model presented in Exercise 5 of Chapter \\(2 .\\) Find the distribution of \\(V_{1}=\\sum_{i=1}^{a} \\sum_{j=1}^{s} \\sum_{k=1}^{t}\\left(\\vec{Y}_{i j .}-\\bar{Y}_{i . .}\\right)^{2}\\). Find the distribution of \\(V_{2}=\\sum_{i=1}^{a} \\sum_{j=1}^{s} \\sum_{k=1}^{t}\\left(Y_{i j k}-\\bar{Y}_{i j} .\\right)^{2}\\). Find the distribution of \\(\\left\\{V_{1} /[a(s-1)]\\right\\} /\\left\\{V_{2} /[a s(t-1)]\\right\\}\\). Consider Exercise 6 of Chapter 2 . Use Corollary 3.1.2(a) to find the distribution of \\(U\\). Use Theorem \\(3.2 .2\\) to show that \\(U\\) and \\(\\bar{Y}\\). are independent. Prove Theorem \\(3.1 .2\\), part \\((2)\\). Use Corollary \\(3.1 .2(\\) a \\()\\) to find the distribution of \\(\\left(\\bar{Y}_{1},-\\bar{Y}_{2}\\right)^{2}\\) from Exercise \\(9 \\mathrm{~b}\\) in Chapter \\(2 .\\) Consider Exercise 11 of Chapter 2 . Use Theorem \\(3.2 .2\\) to show that \\(Y_{1}+2 Y_{2}-Y_{3}\\) is independent of \\(2 Y_{1}^{2}+\\) \\(Y_{2}^{2}+2 Y_{3}^{2}-2 Y_{1} Y_{2}+2 Y_{2} Y_{3}\\) Use Corollary 3.1.2(a) to find a constant \\(c\\) such that \\(c\\left[5 Y_{1}^{2}+2 Y_{2}^{2}+5 Y_{3}^{2}-\\right.\\) \\(\\left.4 Y_{1} Y_{2}+2 Y_{1} Y_{3}+4 Y_{2} Y_{3}\\right]\\) has a central chi-square distribution. Derive the distributions of \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{3} \\mathbf{Y}\\) and \\(\\mathbf{Y}^{\\prime} \\mathbf{A}_{4} \\mathbf{Y}\\) from Example 2.3.2. Calculate the noncentrality parameters \\(\\lambda_{1}, \\ldots, \\lambda_{4}\\) in Example \\(3.4 .1\\). Let the \\(3 \\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{1}, Y_{2}, Y_{3}\\right)^{\\prime} \\sim \\mathrm{N}_{3}\\left(\\alpha \\mathbf{1}_{3}, \\sigma^{2} \\mathbf{I}_{3}\\right)\\) and define \\(Z_{1}=\\left(Y_{1}^{2}+Y_{3}^{2}-2 Y_{1} Y_{3}\\right.\\) and \\(Z_{2}=Y_{1}^{2}+Y_{2}^{2}+Y_{3}^{2}-Y_{1} Y_{2}-Y_{1} Y_{3}-Y_{2} Y_{3} .\\) Find the distributions of \\(Z_{1}\\) and \\(Z_{2}\\). Find the \\(\\mathrm{E}\\left(Z_{i}^{k}\\right)\\) for \\(i=1,2\\) and any positive integer \\(k\\). Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{1}, \\ldots, Y_{n}\\right)^{\\prime} \\sim \\mathrm{N}_{n}\\left(\\alpha \\mathbf{1}_{n}, \\Sigma\\right)\\) where \\(\\Sigma=\\) \\((a-b) \\mathbf{I}_{n}+b \\mathbf{J}_{n}\\). Find the distribution of \\(V=\\sum_{i=1}^{n-1}\\left(Y_{i}-\\bar{Y}^{*}\\right)^{2}\\) where \\(\\bar{Y}^{*}=\\sum_{i=1}^{n-1} Y_{i} /\\) \\((n-1)\\). Find the distribution of \\(\\left(\\bar{Y}^{*}-Y_{n}\\right) / V^{1 / 2}\\). Let the \\(n \\times 1\\) random vector \\(\\mathbf{Y} \\sim \\mathrm{N}_{n}\\left(\\boldsymbol{\\mu}, \\mathbf{I}_{n}\\right) .\\) Let \\(\\mathbf{X}=\\mathbf{A Y}\\) where \\(\\mathbf{A}\\) is an \\(n \\times n\\) orthogonal matrix whose first row is \\(\\boldsymbol{\\mu}^{\\prime} / \\sqrt{\\boldsymbol{\\mu}^{\\prime} \\boldsymbol{\\mu}}\\). Let \\(V=X_{1}^{2}\\left(X_{1}\\right.\\) is the first element of vector \\(\\mathbf{X}\\) ) and \\(U=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}-V\\right)\\). Find the distributions of \\(U\\) and \\(V\\). Are \\(U\\) and \\(V\\) independent? Prove your answer. Let \\(U_{i} \\sim \\chi^{2}\\left(\\lambda_{i}\\right)\\) for \\(i=1,2\\) where \\(U_{1}\\) and \\(U_{2}\\) are independent. Let \\(a\\) and \\(b\\) be two positive constants. Under what conditions is \\(a U_{1}+b U_{2} \\sim c \\chi^{2}(\\lambda) ?\\) Provide the values of \\(c\\) and \\(\\lambda\\). Consider the model \\(Y_{i j}=\\mu_{i}+R(T)_{(i) j}\\) where \\(R(T)_{(i) j} \\sim\\) iid \\(\\mathrm{N}_{1}\\left(0, \\sigma_{R(T)}^{2}\\right)\\) for \\(i=1, \\ldots, 3, j=1, \\ldots, n_{i}\\) with \\(n_{1}=3, n_{2}=4\\), and \\(n_{3}=2\\). Find the distribution of \\(U=\\sum_{i=1}^{3} \\sum_{j=1}^{n_{i}}\\left(Y_{i j}-\\bar{Y}_{i .}\\right)^{2}\\) where \\(\\bar{Y}_{i .}=\\) \\(\\sum_{j=1}^{n_{i}} Y_{i j} / n_{i}\\) Find the expected value of \\(V=\\sum_{i=1}^{3}\\left(\\bar{Y}_{i}-\\bar{Y}^{*}\\right)^{2} \\omega h e r e \\bar{Y}^{*}=\\sum_{i=1} \\bar{Y}_{i} / 3\\) Find the expected value of \\(V=\\sum_{i=1}^{3}\\left(\\bar{Y}_{i .}-\\bar{Y}^{*}\\right)^{2}\\) where \\(\\bar{Y}^{*}=\\sum_{i=1}^{3} \\bar{Y}_{i .} / 3\\). [Hint: Write \\(V=\\overline{\\mathbf{Y}}^{\\prime}\\left(\\mathbf{I}_{3}-\\frac{1}{3} \\mathbf{J}_{3}\\right) \\overline{\\mathbf{Y}}\\) where \\(\\left.\\overline{\\mathbf{Y}}=\\left(\\bar{Y}_{1 .}, \\bar{Y}_{2 .}, \\bar{Y}_{3 .}\\right)^{\\prime} .\\right]\\) (Paired \\(t\\)-Test Problem) Consider an experiment with \\(n\\) experimental units. Suppose two observations are made on each unit. The first observation corresponds to the first level of a fixed factor, the second observation to the second level of the fixed factor. Let \\(Y_{i j}\\) be a random variable representing the \\(j^{\\text {th }}\\) observation on the \\(i^{\\text {th }}\\) experimental unit for \\(i=1, \\ldots, n\\) and \\(j=1,2\\). Let the \\(2 n \\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{11}, Y_{12}, Y_{21}, Y_{22}, \\ldots, Y_{n 1}, Y_{n 2}\\right)^{\\prime}\\). Let \\(\\mathrm{E}\\left(\\mathbf{Y}_{i j}\\right)=\\mu_{j}\\) and \\(\\operatorname{var}\\left(Y_{i j}\\right)=\\sigma^{2}\\) for \\(i=1, \\ldots, n\\) and \\(j=1,2 ;\\) and let \\(\\operatorname{cov}\\left(Y_{i 1}, Y_{i 2}\\right)=\\sigma^{2} \\rho\\) for all \\(i=1, \\ldots, n .\\) Assume \\(\\mathbf{Y} \\sim \\mathrm{N}_{2 n}(\\mu, \\Sigma)\\) Define \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) in terms of \\(\\boldsymbol{\\mu}_{j}, \\sigma^{2}\\), and \\(\\rho\\). (Hint: Use Kronecker products.) Let \\(T=\\bar{D} /\\left(S_{D} / \\sqrt{n}\\right)\\) where \\(D_{i}=Y_{i 1}-Y_{i 2}\\) for \\(i=1, \\ldots, n ; \\bar{D}=\\) \\(\\sum_{i=1}^{n} D_{i} / n ;\\) and \\(S_{D}^{2}=\\sum_{i=1}^{n}\\left(D_{i}-\\bar{D}\\right)^{2} /(n-1) .\\) Find the distribution of \\(T .\\left[\\right.\\) Hint \\(:\\) Start by finding the distribution of \\(\\left.D=\\left(D_{1}, \\ldots, D_{n}\\right)^{\\prime} .\\right]\\) \\(T .\\left[\\right.\\) Hint \\(:\\) Start by finding the distribution of \\(\\left.D=\\left(D_{1}, \\ldots, D_{n}\\right)^{\\prime} .\\right]\\) the \\(6 n \\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{111}, \\ldots, Y_{11 n}, Y_{121}, \\ldots, Y_{12 n}, \\ldots, Y_{131}\\right.\\), \\(\\left.\\ldots, Y_{13 n}, Y_{211}, \\ldots, Y_{21 n}, \\ldots, Y_{221}, \\ldots, Y_{22 n}, Y_{231}, \\ldots, Y_{23 n}\\right)^{\\prime} \\sim \\mathrm{N}_{6 n}\\left(\\mathbf{1}_{2} \\otimes\\right.\\) \\(\\left.\\left(\\mu_{1}, \\mu_{2}, \\mu_{3}\\right)^{\\prime} \\otimes \\mathbf{1}_{n}, \\mathbf{\\Sigma}\\right)\\) where \\[\\begin{aligned} \\boldsymbol{\\Sigma}=&amp; \\sigma_{1}^{2}\\left[\\mathbf{I}_{2} \\otimes \\mathbf{J}_{3} \\otimes \\mathbf{J}_{n}\\right] \\\\ &amp;+\\sigma_{2}^{2}\\left[\\mathbf{I}_{2} \\otimes\\left(\\mathbf{I}_{3}-\\frac{1}{3} \\mathbf{J}_{3}\\right) \\otimes \\mathbf{J}_{n}\\right] \\\\ &amp;+\\sigma_{3}^{2}\\left[\\mathbf{I}_{2} \\otimes \\mathbf{I}_{3} \\otimes \\mathbf{I}_{n}\\right] \\end{aligned}\\] Show \\(\\bar{Y}_{.1 .}-\\bar{Y}_{.2 .}=\\left[(1 / 2) \\mathbf{1}_{2}^{\\prime} \\otimes(1,-1,0) \\otimes(1 / n) \\mathbf{1}_{n}^{\\prime}\\right] \\mathbf{Y}\\) where \\(\\bar{Y}_{. j}=\\) \\(\\sum_{i=1}^{2} \\sum_{k=1}^{n} Y_{i j k} /(2 n)\\) Find the distribution of \\(\\bar{Y}_{.1}-\\bar{Y}_{.2 .}\\). Find the distribution of \\(\\mathbf{Y}^{\\prime}\\left[\\left(\\mathbf{I}_{2}-\\frac{1}{2} \\mathbf{J}_{2}\\right) \\otimes\\left(\\mathbf{I}_{3}-\\frac{1}{3} \\mathbf{J}_{3}\\right) \\otimes \\frac{1}{n} \\mathbf{J}_{n}\\right] \\mathbf{Y}\\). Let the \\(\\left(n_{1}+n_{2}\\right) \\times 1\\) random vector \\(\\mathbf{Y}=\\left(Y_{11}, \\ldots, Y_{1 n_{1}}, Y_{21}, \\ldots, Y_{2 n_{2}}\\right)^{\\prime} \\sim\\) \\(\\mathrm{N}_{n_{1}+n_{2}}(\\mu, \\Sigma)\\) where \\(\\boldsymbol{\\mu}=\\left(\\mu_{1} \\mathbf{1}_{n_{1}}^{\\prime}, \\mu_{2} \\mathbf{1}_{n_{2}}^{\\prime}\\right)^{\\prime}\\) and \\[\\boldsymbol{\\Sigma}=\\left[\\begin{array}{cc} \\sigma_{1}^{2} \\mathbf{I}_{n_{1}} &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\sigma_{2}^{2} \\mathbf{I}_{n_{2}} \\end{array}\\right]\\] If \\(\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}\\), this problem is called the Behrens-Fisher problem. Find the distribution of \\[V=\\frac{\\left(\\bar{Y}_{1 .}-\\bar{Y}_{2 .}\\right)^{2} /\\left(1 / n_{1}+1 / n_{2}\\right)}{\\sum_{i=1}^{2} \\sum_{j=1}^{n_{i}}\\left(Y_{i j}-Y_{i .}\\right)^{2} /\\left(n_{1}+n_{2}-2\\right)}\\] when \\(\\sigma_{1}^{2}=\\sigma_{2}^{2}\\). Describe the distribution of \\(V\\) when \\(\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
